{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Metrics and Optimization\n",
    "\n",
    "Author: Zhaohan Dong\n",
    "\n",
    "Date: Aug 7, 2024\n",
    "\n",
    "In this workshop, we'll go through the three steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaohan_dong/Desktop/intro-dspy/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "\n",
    "# Set up the LM\n",
    "model_name = \"llama3\"\n",
    "ollamaLocal = dspy.OllamaLocal(model=model_name)\n",
    "\n",
    "# Global variable for settings... I don't personally like this for anything other than experiment\n",
    "# dspy.settings.configure(lm=ollama_mistral)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Our Dataset - HotPotQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaohan_dong/Desktop/intro-dspy/venv/lib/python3.12/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from dspy.datasets import HotPotQA\n",
    "\n",
    "# Get HotPotQA dataset\n",
    "hotPotQAdataset = HotPotQA(train_seed=1, train_size=50, eval_seed=2025, dev_size=20, test_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caveat 1: HotPotQA from DSPy by default doesn't define input keys required by Metrics. Only gsm8k does.\n",
    "\n",
    "Inspect the `input_keys` of each `Example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example({'question': 'Pehchaan: The Face of Truth stars Vinod Khanna, Rati Agnihotri and which Indian actress, producer, and former model who also produced the film?', 'answer': 'Raveena Tandon', 'gold_titles': {'Pehchaan: The Face of Truth', 'Raveena Tandon'}}) (input_keys=None),\n",
       " Example({'question': 'What is the name of the person who helped work on Les Sylphides and past away 22 August 1942?', 'answer': 'Michael Fokine', 'gold_titles': {'Michel Fokine', 'Les Sylphides'}}) (input_keys=None),\n",
       " Example({'question': 'Trim is a transitway station in the east end of a city that as of 2016 had a population of what?', 'answer': '934,243', 'gold_titles': {'Ottawa', 'Trim station'}}) (input_keys=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset from HotPotQA\n",
    "hotPotQAdataset.dev[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example({'question': 'Pehchaan: The Face of Truth stars Vinod Khanna, Rati Agnihotri and which Indian actress, producer, and former model who also produced the film?', 'answer': 'Raveena Tandon', 'gold_titles': {'Pehchaan: The Face of Truth', 'Raveena Tandon'}}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the name of the person who helped work on Les Sylphides and past away 22 August 1942?', 'answer': 'Michael Fokine', 'gold_titles': {'Michel Fokine', 'Les Sylphides'}}) (input_keys={'question'}),\n",
       " Example({'question': 'Trim is a transitway station in the east end of a city that as of 2016 had a population of what?', 'answer': '934,243', 'gold_titles': {'Ottawa', 'Trim station'}}) (input_keys={'question'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting input keys\n",
    "trainset = [example.with_inputs(\"question\") for example in hotPotQAdataset.train]\n",
    "devset = [example.with_inputs(\"question\") for example in hotPotQAdataset.dev]\n",
    "testset = [example.with_inputs(\"question\") for example in hotPotQAdataset.test]\n",
    "devset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hotPotQAdataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define Module Signature (I/O of the Module)\n",
    "\n",
    "DSPy signature inherits Pydantic `BaseModel`.\n",
    "\n",
    "A weird thing is that DSPy takes instruction prompt as a docstring in class-based Signature (dspy/signatures/signature.py:89-91):\n",
    "\n",
    "``` python\n",
    "@property\n",
    "    def instructions(cls) -> str:\n",
    "        return getattr(cls, \"__doc__\", \"\")\n",
    "```\n",
    "\n",
    "Here we use `make_signature()` to create a signature. We will demonstrate the class-based Signature later in Metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringSignature(question -> answer\n",
       "    instructions='Given the fields `question`, produce the fields `answer`.'\n",
       "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
       "    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.signatures.signature import make_signature\n",
    "\n",
    "# dspy/signatures/signature.py signature: The signature format, specified as \"input1, input2 -> output1, output2\"\n",
    "# Note how the \"question\" and \"answer\" matches the input and output keys in HotPotQA dataset\n",
    "qaSignature = make_signature(\"question -> answer\")\n",
    "qaSignature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define Module\n",
    "\n",
    "Out of the box DSPy supports:\n",
    "- `dspy.Predict` - The most basic one\n",
    "- `dspy.ChainOfThought`\n",
    "- `dspy.ProgramOfThought`\n",
    "- `dspy.reAct`\n",
    "- `dspy.MultiChainComparison`\n",
    "- `dspy.majority`\n",
    "\n",
    "[Official Documentation on default modules](https://dspy-docs.vercel.app/docs/building-blocks/modules#what-other-dspy-modules-are-there-how-can-i-use-them)\n",
    "\n",
    "Here we demonstrate how to define a child class of the default modules adapted from official tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prog = ChainOfThought(StringSignature(question -> answer\n",
       "    instructions='Given the fields `question`, produce the fields `answer`.'\n",
       "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
       "    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n",
       "))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CoT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(signature=qaSignature)  # In official examples, they use a string instead of StringSignature. But this is more explicit\n",
    "    \n",
    "    # When we call the module instance, it calls the forward method (dspy/primitives/program.py:25-26)\n",
    "    def forward(self, question):\n",
    "        return self.prog(question=question)\n",
    "    \n",
    "cot = CoT()\n",
    "cot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define a Customized Metric\n",
    "\n",
    "We need to define a metric to assess and optimize our Module.\n",
    "\n",
    "The way we define the metric is to pass the prediction back to an LLM and assess:\n",
    "- Is the response truthful? (1 point)\n",
    "- Is the rationale correct? (1 point)\n",
    "\n",
    "`metric = (max_word_output - output_length) / max_word_output`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dspy-docs.vercel.app/docs/building-blocks/metrics\n",
    "\n",
    "# Creating class-based signatures for automatic assessments\n",
    "# The docstrings here would be part of the instruction prompt\n",
    "\n",
    "from dspy import Example, Prediction\n",
    "\n",
    "class AssessTruthfulness(dspy.Signature):\n",
    "    \"\"\"Assess the truthfulness of response. Answer with only YES or NO\"\"\"\n",
    "\n",
    "    assessed_prediction = dspy.InputField()\n",
    "    assessment_question = dspy.InputField()\n",
    "    assessment_answer = dspy.OutputField(desc=\"Answer with Yes or No, do not repeate the question\")\n",
    "\n",
    "# Bad signature with bad prompts\n",
    "class BadAssessTruthfulness(dspy.Signature):\n",
    "    \"\"\"Assess the truthfulness of response.\"\"\"\n",
    "\n",
    "    assessed_prediction = dspy.InputField()\n",
    "    assessment_question = dspy.InputField()\n",
    "    assessment_answer = dspy.OutputField(desc=\"Answer with Yes or No, do not repeate the question\")\n",
    "\n",
    "\n",
    "# Define metrics\n",
    "\n",
    "max_word_output = 4096 # demo purpose only, should use token\n",
    "\n",
    "# The metric with prompts that controls hallucination\n",
    "def qaReasoningMetric(gold: Example, pred: Prediction, trace=None) -> float:\n",
    "    # You have to ensure gold has the right keys\n",
    "    # In our case, HotPotQA has question and answer keys\n",
    "    try:\n",
    "        question, answer = gold.question, gold.answer\n",
    "    except AttributeError as e:\n",
    "        raise AttributeError(f\"{e}. gold must have question and answer attributes.\")\n",
    "\n",
    "    # The keys of pred also needs to match the input prediction\n",
    "    try:\n",
    "        answer, rationale = pred.answer, pred.rationale\n",
    "    except AttributeError as e:\n",
    "        raise AttributeError(f\"{e}. pred must have answer and rationale attributes.\")\n",
    " \n",
    "    assess_answer_prompt = f\"The text should answer `{question}` with `{answer}`. Does the assessed prediction answered correctly? Answer with Yes or No only.\"\n",
    "    \n",
    "    # You can set a different model to assess the model\n",
    "    with dspy.context(lm=ollamaLocal):\n",
    "        correct =  dspy.Predict(AssessTruthfulness)(assessed_prediction=answer, assessment_question=assess_answer_prompt)\n",
    "        \n",
    "    correct = correct.assessment_answer.lower() == 'yes'\n",
    "    score = (max_word_output - len(rationale + answer)) / max_word_output if correct else 0  # No point in being reasonable if the question is not answered correctly\n",
    "\n",
    "    if trace is not None: return score >= 1\n",
    "    return score\n",
    "\n",
    "# A problematic metric because the LLM will hallucinate and not output a binary yes/no answer\n",
    "def badQAReasoningMetric(gold: Example, pred: Prediction, trace=None) -> float:\n",
    "    question, answer  = gold.question, gold.answer\n",
    "    answer, rationale = pred.answer, pred.rationale\n",
    "\n",
    "    assess_answer_prompt = f\"The text should answer `{question}` with `{answer}`. Does the assessed prediction answered correctly?\"\n",
    "    \n",
    "    with dspy.context(lm=ollamaLocal):\n",
    "        correct =  dspy.Predict(BadAssessTruthfulness)(assessed_prediction=answer, assessment_question=assess_answer_prompt)\n",
    "\n",
    "    # Print the assessment result from LLM for demo\n",
    "    print(correct)\n",
    "\n",
    "    correct = correct.assessment_answer.lower() == 'yes'\n",
    "    score = (max_word_output - len(rationale + answer)) / max_word_output if correct else 0\n",
    "\n",
    "    if trace is not None: return score >= 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.920166015625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assess with right metric\n",
    "with dspy.context(lm=ollamaLocal):\n",
    "    pred = cot(devset[12].question)\n",
    "qaReasoningMetric(gold=devset[12], pred=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    assessment_answer='Assessed Prediction: Heaven 17\\nAssessment Question: The text should answer \"Twelve Inches is a compilation album by which 1980s British band?\" with `Heaven 17`.\\nAssessment Answer: Yes'\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM assessment that cannot calculate metrics based on (result is yes, but not \"yes\")\n",
    "badQAReasoningMetric(gold=devset[12], pred=pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate using Metric and Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "# Get metrics with our testset data, devset is actually testset in this case\n",
    "evaluator = Evaluate(devset=testset, metric=qaReasoningMetric, num_threads=4, display_progress=True, display_table=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.096923828125 / 20  (40.5): 100%|██████████| 20/20 [00:51<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.096923828125 / 20  (40.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with dspy.context(lm=ollamaLocal):\n",
    "    evaluator(program=cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize using teleprompter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:40<00:00,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 0 full traces after 50 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "# Set up the optimizer: we want to \"bootstrap\" (i.e., self-generate) 4-shot examples of our CoT program.\n",
    "config = dict(metric=qaReasoningMetric, max_bootstrapped_demos=8, max_labeled_demos=8)\n",
    "\n",
    "# Optimize! Use the `qaReasoningMetric` here. In general, the metric is going to tell the optimizer how well it's doing.\n",
    "teleprompter = BootstrapFewShot(**config)\n",
    "\n",
    "with dspy.context(lm=ollamaLocal):\n",
    "    optimized_cot = teleprompter.compile(CoT(), trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.515869140625 / 20  (22.6): 100%|██████████| 20/20 [01:49<00:00,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.515869140625 / 20  (22.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with dspy.context(lm=ollamaLocal):\n",
    "    evaluator(program=optimized_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
